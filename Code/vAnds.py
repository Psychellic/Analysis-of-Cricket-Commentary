# -*- coding: utf-8 -*-
"""NLP_V&S_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CDZMXqnOVsyLIUdLWGHKrK7x8zss_MMP
"""

from transformers import BertTokenizer, BertForMaskedLM
from transformers import pipeline
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('vader_lexicon')
# Load pre-trained BERT model and tokenizer
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForMaskedLM.from_pretrained(model_name)

# Define the summarization pipeline
summarization_pipeline = pipeline("summarization")

def summarize_over(over):
    over_text = " ".join(over)
    summary = summarization_pipeline(over_text, max_length=100, min_length=10, do_sample=False)[0]['summary_text']
    return summary

def highlights(text_lines, n):
    interesting_scores = calculate_interesting_scores(text_lines)
    sorted_scores = sorted(interesting_scores.items(), key=lambda x: x[1], reverse=True)
    top_n_overs = dict(sorted_scores[:n])
    highlights = {}
    for over_number in top_n_overs.keys():
        over_text = text_lines[over_number - 1]
        summary = summarize_over(over_text)
        highlights[over_number] = summary
    return highlights

def calculate_interesting_scores(text_lines):
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def preprocess_text(text_list):
        preprocessed_text = []
        for text in text_list:
            tokens = word_tokenize(text.lower())
            tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]
            preprocessed_text.append(tokens)
        return preprocessed_text

    def wordscore(text_lines):
        scores = {}
        for i, sublist in enumerate(text_lines):
            preprocessed_commentary = preprocess_text(sublist)
            word2vec_model = Word2Vec(preprocessed_commentary, vector_size=100, window=5, min_count=1, workers=4)
            cricket_related_terms = ["six", "four", "boundary", "wicket", "catch", "stump"]
            related_words = {}
            for term in cricket_related_terms:
                if term in word2vec_model.wv.key_to_index:
                    similar_words = word2vec_model.wv.most_similar(term, topn=5)
                    related_words[term] = [word for word, _ in similar_words]
            count_related_words = sum(len(words) for words in related_words.values())
            scores[i + 1] = count_related_words
        return scores

    def sentimentscore(text_lines):
        sia = SentimentIntensityAnalyzer()
        over_sentiment_scores = {}
        for over_number, over in enumerate(text_lines, start=1):
            over_text = ' '.join(over)
            sentiment_score = sia.polarity_scores(over_text)["compound"]
            over_sentiment_scores[over_number] = sentiment_score
        return over_sentiment_scores

    sentiment_scores = sentimentscore(text_lines)
    word_scores = wordscore(text_lines)

    min_sentiment = min(sentiment_scores.values())
    max_sentiment = max(sentiment_scores.values())
    sentiment_range = max_sentiment - min_sentiment
    normalized_sentiments = {over: (score - min_sentiment) / sentiment_range
                             for over, score in sentiment_scores.items()}

    min_word_score = min(word_scores.values())
    max_word_score = max(word_scores.values())
    word_score_range = max_word_score - min_word_score
    normalized_word_scores = {over: (score - min_word_score) / word_score_range
                              for over, score in word_scores.items()}

    interesting_scores = {over: normalized_sentiments[over] + normalized_word_scores[over]
                          for over in sentiment_scores.keys()}

    return interesting_scores

def v_and_s(text_lines, n):
    interesting_scores = calculate_interesting_scores(text_lines)
    highlighted_overs = highlights(text_lines, n)
    sorted_highlights = sorted(highlighted_overs.items(), key=lambda x: interesting_scores[x[0]], reverse=True)
    print("Highlighted Overs:")
    for over, summary in sorted_highlights:
        print(f"Over {over}: {summary}\n")

